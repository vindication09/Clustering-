{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducton to Python Session 8 \n",
    "\n",
    "## The Data Science Workflow \n",
    "\n",
    "Our last session wrapped up Chapter 4 of the Social Network Analysis book. Today's session will be the start of something new, specifically the modeling workflow. Todays first lesson will be more conceptual as we will cover some of the more common terms used with modeling. The data science workflow can be broken down into several subsections. We are going to go into detail regarding what each of these sub sections stands for. We will cover the first three sections today.\n",
    "\n",
    "In terms of material, we will be using the \"Data Science for Business\" book. It is a popular book that is used in many data science programs. I used it myself for my Data Management class. \n",
    "\n",
    "* Problem Identification \n",
    "* Data Gathering \n",
    "* Data Cleaning \n",
    "* Exploration \n",
    "* Feature Engineering \n",
    "* Normalizations and Transformations \n",
    "* Model\n",
    "* Validate \n",
    "* Deploy/Interpret "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Identification \n",
    "\n",
    "This is typically considered to be the most difficult step of the data science workflow. A well defined problem statement can make or break the entire project. This step determines if you have something that models garbage in or garbage out vs something with real insight. Here are some guidelines to identifying the problem. \n",
    "\n",
    " - What are you trying to solve? \n",
    " \n",
    " Are you trying to predict an outcome? Are you trying to classify data into groups? Are you trying to measure performance? Are you trying to capture the relationship between two entities? Are we trying to model some segment? There are more questions that we can come up with but this sort of brain storming is crucial. Once you have an idea of what it is you want to solve, then the next step is to source your data. \n",
    " \n",
    "## Data Gathering \n",
    "\n",
    "Sourcing your data is where domain knowledge comes into play. You want to be able to gather data from one or more sources that can best capture the variables related to the problem you are trying to solve. This step will most likely have you interacting with a database and using some variation of SQL. It's less uncommon to be able to find a csv/excel workbook that already has clean data and variables. It will be your job to create this clean data and find the right variables. During the data gathering phase, you should develop an intuition to what kind of data structure you want to work with. By data structure, this includes explicitly defined rows and columns, data format (JSON, TXT, CSV...etc.), and storage. By defined rows, I mean knowing what each row of your data should represent? Does each row represent a user, a timestamp, or an experiment? Can rows be unique or duplicated? If rows are duplicated, did you impliment some sort of nesting for the other variables in your data frame? \n",
    "\n",
    "## Data Cleaning \n",
    "\n",
    "This is where 70-80 percent of time will be spent. Real data is never clean and requires some sort of engineering in order to make it useable. The overall goal in data cleaning is to maximize the accuracy of a dataset without getting rid of information. Most analysts start by examining the distribution of missing data by column. Treating missing data is debated frequently in data science forums. Many people argue that a variable should be deleted if it is missing more than 30 percent of their values, however counter arguments insist that variables could be too valuable to simply delete from the data. This is when good judgement and creativity shine through. What are some ways we can deal with this? \n",
    "\n",
    "Imputation is a common method that allows us to fill in missing data. If a variable is continous, then missing data can be filled in using the median or mean of the non-missing values. Mean vs Median can be determined after you get a sense of the skewness in your data. A general rule of thumb is that symmetrically distributed values can be imputed with the mean, otherwise use the median. This is where knowledge of statistics comes in handy regarding how to handle different distributions. \n",
    "\n",
    "Imputation can also be done on categorical values. The mode of a categorical column is an easy approach to imputing categorical values if the missing data threshold is \"low\", however if we are missing half our values and use the mode of the non-missing values, we will end up polluting the variable so there needs to be a balance when imputing categorical data. Sometimes, you will find that you might have to manually impute categorical values. You need to lean on your domain knoweledge in this instance to see if its even worth it. \n",
    "\n",
    "Another element data cleaning is changing the names of columns. Sometimes we get data with column names such as X1, X2, or Variable_1, Variable_2, etc. There will be some cases where you'll need to inspect the actual data values themselves, for example, lets say we have variable called \"budget\" but notice that some of our values are negative. Does this make sense? Lets say we have another variable called \"length in weeks\" and also get some negative numbers. We really can't have negative time. Sometimes there are errors with the actual data collection that are outside our own control and from my experience, they don't account for more than 10 % of the data. Sometimes, we don't see these issues until we begin exploring the data but it is good to keep in mind.  \n",
    "\n",
    "In terms of variables in columns, there may be instances where we might need to change values all together or potentially roll up some values into less categories. Such examples are when we use CASE WHEN to classify several granular categories into more practical handful of categories. While this is going on, we might also add a label to our data if none has been assigned for supervised learning type problems. A label is a vector which labels a data point as class A, class B...etc. In our cases, our labels could be testgroup and controlgroup. For salesforce, the label is win or losses. \n",
    "\n",
    "## Exploration \n",
    "\n",
    "Exploration is going to play an important role in driving decisions for the feature selection process. There are several elements that come into play when exploring a data set. We use exploration to find relationships among our variables, patterns, and distributions. We use all of this information gathered to make informed decisions in terms of our modeling process. I will talk about some of the most common elements used in exploration. \n",
    "\n",
    "The first is a statistical summary of your columns. The summary allows you to get a sense of the mean, medians, max, mins, and modes for your variables. Both R and python are able to differentiate between data types when producing summary results. Summaries are a great tool to be able to keep note of columns where irregularities might exist. The max and min of a variable can be useful in finding which variables could have unrealistic values. If you see a large difference between the mean and median, it is probably due to some skewness so perhaps you want to keep some transformations in mind. \n",
    "\n",
    "Correlations are also performed in the exploration step. We want to be able to see what variables are correlated with each other and what variables are correlated with the outcome variable if dealing with a supervised learning problem. Knowledge of statistics is required to understand how correlations can impact a model. Too many correlated variables will produce multi-colinearity, which simply means that one or more predictor can be represented as a linear combination of one or more other predictors. Modeling multi-colinearity in data will universally produce nonsense. \n",
    "\n",
    "During your exploration you will also want to identify and remove variables that may be biased and cause your algorithm to make generalizations on a subset and fail miserably during prediction. This usually comes in the form of finding variables that are perfectly correlated with the response variable. This is common in supervised learning. What does this mean? Lets say our outcome variable is a binary 1 or 0. Our data is every potential customer that wants to buy a car online. If they bought one, then their label is 1, otherwise it is 0. If I have a predictor variable such as \"car model purchased\" then this predictor would be perfectly correlated with our outcome variable, thus causing any sort of modeling to \"overfit\" and not give anything meaningful. In this scenerio, I wanted to find the most important features that influence a car purchase such as credit history, income, ...etc. \n",
    "\n",
    "By the end of exploration, a user should know where to apply transforms based on distributions, flag correlated variables, and have a better understanding of relationships in the data. \n",
    "\n",
    "Next week, I want to come up with an example of what the first three steps look like while moving on into the feature engineering and modeling. There is a lot to talk about here. \n",
    "\n",
    "## Homework \n",
    "\n",
    "Read chapter 1 of the Data Science for Business book if you have not done so already.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
